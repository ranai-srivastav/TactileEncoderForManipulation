{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd8d275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These two below should match\n",
      "/ocean/projects/cis260031p/shared/temu_conda/bin/python3\n",
      "/ocean/projects/cis260031p/shared/temu_conda/bin/python3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These two above should match\n"
     ]
    }
   ],
   "source": [
    "print(\"These two below should match\")\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "!which python3\n",
    "print(\"These two above should match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1739030",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL.Image import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fde0f52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineTactileEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rgb_freq: int, \n",
    "        tactile_freq: int,\n",
    "        ft_freq: int,\n",
    "        vision_resnet_emb_size: int,\n",
    "        tactile_resnet_emb_size: int,\n",
    "        ft_emb_size: int,\n",
    "        gripper_emb_size: int,\n",
    "        model_emb_size: int,\n",
    "        cell_state_size: int,\n",
    "        hidden_state_size: int):\n",
    "        \n",
    "        super(BaselineTactileEncoder, self).__init__()\n",
    "        self.vision_encoder = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.tactile_encoder = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.vision_resnet_emb_size = vision_resnet_emb_size\n",
    "        self.tactile_resnet_emb_size = tactile_resnet_emb_size\n",
    "        self.model_emb_size = model_emb_size\n",
    "        self.ft_emb_size = ft_emb_size\n",
    "        self.gripper_emb_size = gripper_emb_size\n",
    "        self.hidden_state_size = hidden_state_size\n",
    "        \n",
    "        self._rgb_freq = rgb_freq \n",
    "        self._tactile_freq = tactile_freq\n",
    "        self._ft_freq = ft_freq\n",
    "        \n",
    "        self.resnet_weights = ResNet50_Weights.DEFAULT\n",
    "        self.resnet_transforms = self.resnet_weights.transforms()\n",
    "        # PIL IMAGE (B, C, H, W) -> \n",
    "        # resized to (B, C, 256, 256) -> \n",
    "        # normalized(values[0 to 1], mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "        ## Vision part\n",
    "        self.vision_encoder.fc = nn.Identity() # type: ignore[assignment]\n",
    "\n",
    "\n",
    "        ## Tactile part\n",
    "        self.tactile_encoder.fc = nn.Identity() # type: ignore[assignment]\n",
    "        \n",
    "        self._pre_lstm_dim = (self._rgb_freq    * self.vision_resnet_emb_size + \n",
    "                            self._tactile_freq * self.tactile_resnet_emb_size + \n",
    "                            self._ft_freq      * self.ft_emb_size + \n",
    "                            self._gripper_freq * self.gripper_emb_size)\n",
    "\n",
    "        ## pre-lstm projection\n",
    "        self.fc1 = nn.Linear(in_features=self._pre_lstm_dim, \n",
    "                             out_features=self.model_emb_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=2*self.model_emb_size, \n",
    "                            hidden_size=self.hidden_state_size, \n",
    "                            num_layers=6,\n",
    "                            bidirectional=False, \n",
    "                            batch_first=True)\n",
    "\n",
    "        self.fc2 = nn.Linear(input_size=self.hidden_state_size, \n",
    "                            output_size=self.hidden_state_size//4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn_fc2 = nn.BatchNorm1d(self.hidden_state_size//4)\n",
    "        self.fc3 = nn.Linear(input_size=self.hidden_state_size//4, \n",
    "                            output_size=1)\n",
    "\n",
    "    def forward(self, \n",
    "                vision_img:Image,  #---------(f1*T,3,H,W), sampled at f1 fps\n",
    "                tactile_img:Image, #---------(f4*T,3,H,W), sampled at f4 fps\n",
    "                ft_data:torch.Tensor, #------(f2*T,6), sampled at f2 fps\n",
    "                gripper_force:torch.Tensor #--(1, 1), sampled at f3 fps\n",
    "                )->torch.Tensor: #---------->> predicted slip probability for time T+1\n",
    "        \n",
    "        assert vision_img.shape[0] == self._rgb_freq * vision_img.shape[1] // 3, f\"Expected vision_img shape ({self._rgb_freq},3,H,W), got {vision_img.shape}\"\n",
    "        assert tactile_img.shape[0] == self._tactile_freq * tactile_img.shape[1] // 3, f\"Expected tactile_img shape ({self._tactile_freq},3,H,W), got {tactile_img.shape}\"\n",
    "        assert ft_data.shape[0] == self._ft_freq * ft_data.shape[1] // 6, f\"Expected ft_data shape ({self._ft_freq},6), got {ft_data.shape}\"\n",
    "        assert gripper_force.shape[0] == self._gripper_freq * gripper_force.shape[1] // 2, f\"Expected gripper_force shape ({self._gripper_freq},2), got {gripper_force.shape}\"\n",
    "        \n",
    "        preprocd_vision = self.resnet_transforms(vision_img)\n",
    "        preprocd_tactile = self.resnet_transforms(tactile_img)\n",
    "\n",
    "        vision_emb = self.vision_encoder(preprocd_vision) # (f1*T, vision_resnet_emb_size)\n",
    "        vision_emb = torch.flatten(vision_emb, start_dim=1) # (f1*T*vision_resnet_emb_size)\n",
    "        \n",
    "        tactile_emb = self.tactile_encoder(preprocd_tactile) # (f4*T, tactile_resnet_emb_size)\n",
    "        tactile_emb = torch.flatten(tactile_emb, start_dim=1) # (f4*T*tactile_resnet_emb_size)\n",
    "\n",
    "        ft_emb = torch.flatten(ft_data, start_dim=1) # (f2*T, 6)\n",
    "        gripper_emb = torch.repeat(gripper_force, (ft_data.size[0]))\n",
    "\n",
    "        ## concatenate all the embeddings\n",
    "        combined_emb = torch.cat((vision_emb, tactile_emb, ft_emb, gripper_emb), dim=1) # (f1*T*vision_resnet_emb_size + f4*T*tactile_resnet_emb_size + f2*T*6 + f3*T*2)\n",
    "\n",
    "        assert combined_emb.shape[1] == self._pre_lstm_dim, f\"Expected combined_emb shape ({self._pre_lstm_dim}), got {combined_emb.shape[1]}\"\n",
    "        \n",
    "        fc1_out = self.fc1(combined_emb) # (model_emb_size)\n",
    "        lstm_h, lstm_c = self.lstm(fc1_out)\n",
    "        # lstm_h shape: (batch_size, NUM_LSTM_LAYERS, hidden_state_size)\n",
    "        # lstm_c shape: (batch_size, NUM_LSTM_LAYERS, hidden_state_size)\n",
    "        lstm_out = torch.concat(lstm_h[:, -1, :], lstm_c[:, -1, :]) # (batch_size, hidden_state_size)\n",
    "        fc2_out = self.fc2(lstm_out) # (hidden_state_size//4)\n",
    "        fc2_out = self.relu(fc2_out)\n",
    "        fc2_out = self.bn_fc2(fc2_out)\n",
    "        \n",
    "        fc3_out = self.fc3(fc2_out) # (1)\n",
    "        slip_prob = torch.sigmoid(fc3_out) # (1)\n",
    "        return slip_prob\n",
    "        \n",
    "        \n",
    "        \n",
    "        # TODO ranais, For Feb 14th\n",
    "        # 1. Need to add relu, dropout, bn, and complete forward\n",
    "        # 2. Need to figure out what all the shapes will be and try to overfit with a limited set of images\n",
    "        # 3. Does the current architecture make sense? is the flattening correct? Do we need to do some sort of pooling instead of flattening? Do we need SOO many fc layers?\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd1d399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (temu_conda)",
   "language": "python",
   "name": "temu_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
