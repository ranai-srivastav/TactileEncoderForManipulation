{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde0f52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class BaselineTactileEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vision_resnet_emb_size: int,\n",
    "        tactile_resnet_emb_size: int,\n",
    "        ft_emb_size: int,\n",
    "        gripper_emb_size: int,\n",
    "        model_emb_size: int,\n",
    "        cell_state_size: int,\n",
    "        hidden_state_size: int):\n",
    "        \n",
    "        super(BaselineTactileEncoder, self).__init__()\n",
    "        self.vision_encoder = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.tactile_encoder = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.vision_resnet_emb_size = vision_resnet_emb_size\n",
    "        self.tactile_resnet_emb_size = tactile_resnet_emb_size\n",
    "        self.model_emb_size = model_emb_size\n",
    "        self.ft_emb_size = ft_emb_size\n",
    "        self.gripper_emb_size = gripper_emb_size\n",
    "        self.hidden_state_size = hidden_state_size\n",
    "        \n",
    "        self.resnet_weights = ResNet50_Weights.DEFAULT\n",
    "        self.resnet_transforms = self.resnet_weights.transforms()\n",
    "        # PIL IMAGE (B, C, H, W) -> \n",
    "        # resized to (B, C, 256, 256) -> \n",
    "        # normalized(values[0 to 1], mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "        ## Vision part\n",
    "        self.vision_encoder.fc = nn.Identity()\n",
    "\n",
    "        ## Tactile part\n",
    "        self.tactile_encoder.fc = nn.Identity()\n",
    "\n",
    "        ## pre-lstm projection\n",
    "        self.fc1 = nn.Linear(input_size=self.vision_resnet_emb_size + self.tactile_resnet_emb_size + self.ft_emb_size * 6 + self.gripper_emb_size * 2, \n",
    "                             output_size=self.model_emb_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.model_emb_size, \n",
    "                            hidden_size=self.hidden_state_size, \n",
    "                            num_layers=6,\n",
    "                            bidirectional=True, \n",
    "                            batch_first=True)\n",
    "\n",
    "        self.fc2 = nn.Linear(input_size=self.hidden_state_size, \n",
    "                            output_size=self.self.hidden_state_size//4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn = nn.BatchNorm1d(self.hidden_state_size//4)\n",
    "        self.fc3 = nn.Linear(input_size=self.hidden_state_size//4, \n",
    "                            output_size=1)\n",
    "\n",
    "    def forward(self, \n",
    "                vision_img:Image,  #---------(f1*T,3,H,W), sampled at f1 fps, for T seconds\n",
    "                tactile_img:Image, #---------(f4*T,3,H,W), sampled at f4 fps, for T seconds\n",
    "                ft_data:torch.Tensor, #------(f2*T,6), sampled at f2 fps, for T seconds\n",
    "                gripper_data:torch.Tensor #--(f3*T,2), sampled at f3 fps, for T seconds\n",
    "                )->torch.Tensor: #---------->> predicted slip probability for this timestep\n",
    "        \n",
    "        preprocd_vision = self.resnet_transforms(vision_img)\n",
    "        preprocd_tactile = self.resnet_transforms(tactile_img)\n",
    "\n",
    "        vision_emb = self.vision_encoder(preprocd_vision) # (f1*T, vision_resnet_emb_size)\n",
    "        vision_emb = torch.flatten(vision_emb, start_dim=1) # (f1*T*vision_resnet_emb_size)\n",
    "        \n",
    "        tactile_emb = self.tactile_encoder(preprocd_tactile) # (f4*T, tactile_resnet_emb_size)\n",
    "        tactile_emb = torch.flatten(tactile_emb, start_dim=1) # (f4*T*tactile_resnet_emb_size)\n",
    "\n",
    "        ft_emb = torch.flatten(ft_data, start_dim=1) # (f2*T, 6)\n",
    "        gripper_emb = torch.flatten(gripper_data, start_dim=1) # (f3*T, 2)\n",
    "\n",
    "        ## concatenate all the embeddings\n",
    "        combined_emb = torch.cat((vision_emb, tactile_emb, ft_emb, gripper_emb), dim=1) # (f1*T*vision_resnet_emb_size + f4*T*tactile_resnet_emb_size + f2*T*6 + f3*T*2)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
