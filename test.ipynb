{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd8d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"These two below should match\")\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "!which python3\n",
    "print(\"These two above should match\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "x6wc9oqhl9m",
   "source": "# =============================================================================\n# SECTION 0 — CONFIG  (change these to switch experiments)\n# =============================================================================\n\nROOT_DIR     = \"/ocean/projects/cis260031p/shared/dataset/Gelsight\"\nSUBSAMPLE    = 0.05        # fraction of dataset to load  (0.05 ≈ 25 samples — fast)\nBATCH_SIZE   = 4\nL_MAX        = 20          # max seconds per episode\nSPLIT        = 'object'    # 'object' | 'pose' | 'random'\nTEST_OBJECTS = ['mug', 'bowl']\nTEST_POSES   = [1, 2, 3, 4, 5]\nSIGMA        = 1.0         # DRS target S≠/S= ratio\nSAMPLE_IDX   = 0           # which sample to use for per-sample inspection cells",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ev9iw4j3grb",
   "source": "# =============================================================================\n# SECTION 1 — IMPORTS & CONSTANTS\n# =============================================================================\n\nimport sys, os, random\nfrom pathlib import Path\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, Subset\n\n# Add project dir to path so imports work regardless of cwd\nsys.path.insert(0, str(Path(\"__file__\").resolve().parent)\n                if \"__file__\" in dir() else os.getcwd())\n\nimport dataloader as _dl\nfrom dataloader import (\n    PoseItDataset, split_by_object, split_by_pose,\n    uniform_random_split, collate_variable_length,\n    F1, F2, FT_DIM, GR_DIM,\n    _parse_folder_name, _read_stages, _read_labels,\n    _read_csv_timeseries, _list_image_files,\n    _sample_bucket, _sample_image_bucket,\n    _load_image, _build_sample, IMG_TRANSFORM, LABEL_MAP,\n)\nfrom model import GraspStabilityLSTM\nfrom sampler import DRSSampler\n\n# Apply the episode length cap BEFORE constructing any dataset\n_dl.L = L_MAX\n\nprint(f\"F1={F1}  (image frames/sec)\")\nprint(f\"F2={F2}  (sensor readings/sec)\")\nprint(f\"FT_DIM={FT_DIM}  (F2*6 = {F2}*6)\")\nprint(f\"GR_DIM={GR_DIM}  (F2*2 = {F2}*2)\")\nprint(f\"L_MAX={_dl.L}  (max seconds per episode)\")\nprint(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "6efx4utt2uj",
   "source": "# =============================================================================\n# SECTION 2 — SINGLE SAMPLE: RAW FILE INSPECTION\n# =============================================================================\n\nall_dirs = sorted(d for d in Path(ROOT_DIR).iterdir() if d.is_dir())\nsample_dir = all_dirs[SAMPLE_IDX]\nprint(f\"Inspecting: {sample_dir.name}\\n\")\n\n# --- Parsed metadata from folder name ---\nmeta = _parse_folder_name(sample_dir.name)\nprint(\"Folder name metadata:\")\nfor k, v in meta.items():\n    print(f\"  {k:12s}: {v}\")\n\n# --- stages.csv ---\nstages = _read_stages(sample_dir / 'stages.csv')\nprint(f\"\\nstages.csv ({len(stages)} entries):\")\nfor k, v in sorted(stages.items(), key=lambda x: x[1]):\n    print(f\"  {k:15s}: {v}\")\n\n# --- label.csv ---\nlabels = _read_labels(sample_dir / 'label.csv')\nprint(f\"\\nlabel.csv:\")\nfor k, v in labels.items():\n    num = LABEL_MAP.get(v, -1)\n    print(f\"  {k:12s}: {v!r}  → {num}\")\n\n# --- Temporal window ---\nt_grasp     = stages.get('grasping', stages.get('grasp'))\nt_pose      = stages.get('pose')\nt_stability = stages.get('stability')\nt_retract   = stages.get('retract')\nseconds     = list(range(t_grasp, t_stability))\nif _dl.L is not None:\n    seconds = seconds[:_dl.L]\nprint(f\"\\nTemporal window  (phase='grasp+pose'):\")\nprint(f\"  t_grasp={t_grasp}, t_pose={t_pose}, t_stability={t_stability}, t_retract={t_retract}\")\nprint(f\"  seconds = range({t_grasp}, {t_stability}) = {len(seconds)} steps\")\nprint(f\"  clipped to L={_dl.L}: {len(seconds)} steps remain\")\n\n# --- File counts ---\nft_ts, ft_val     = _read_csv_timeseries(sample_dir / 'f_t.csv',     time_col=0)\ngr_ts, gr_val     = _read_csv_timeseries(sample_dir / 'gripper.csv', time_col=0)\ngel_triples       = _list_image_files(sample_dir / 'gelsight')\nrgb_triples       = _list_image_files(sample_dir / 'rgb')\n\nwindow_mask_ft  = (ft_ts >= t_grasp) & (ft_ts < t_stability)\nwindow_mask_gr  = (gr_ts >= t_grasp) & (gr_ts < t_stability)\ngel_in_window   = [(ts, fi, p) for ts, fi, p in gel_triples if t_grasp <= ts < t_stability]\nrgb_in_window   = [(ts, fi, p) for ts, fi, p in rgb_triples if t_grasp <= ts < t_stability]\n\nprint(f\"\\nFile inventory (full episode):\")\nprint(f\"  GelSight frames  : {len(gel_triples):5d}  ({len(gel_in_window)} in grasp+pose window)\")\nprint(f\"  RGB frames       : {len(rgb_triples):5d}  ({len(rgb_in_window)} in grasp+pose window)\")\nprint(f\"  F/T rows         : {len(ft_ts):5d}  ({window_mask_ft.sum()} in grasp+pose window)\")\nprint(f\"  Gripper rows     : {len(gr_ts):5d}  ({window_mask_gr.sum()} in grasp+pose window)\")\n\n# Per-second frame counts in the window\nprint(f\"\\nPer-second frame counts:\")\nprint(f\"  {'Second':>10}  {'GelSight':>9}  {'RGB':>5}  {'FT rows':>8}  {'Grip rows':>10}\")\nfor sec in seconds[:6]:\n    n_gel = sum(1 for ts, _, _ in gel_in_window if ts == sec)\n    n_rgb = sum(1 for ts, _, _ in rgb_in_window if ts == sec)\n    n_ft  = int((ft_ts == sec).sum())\n    n_gr  = int((gr_ts == sec).sum())\n    print(f\"  {sec:>10}  {n_gel:>9}  {n_rgb:>5}  {n_ft:>8}  {n_gr:>10}\")\nif len(seconds) > 6:\n    print(f\"  ... ({len(seconds)-6} more seconds)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "25v0ppydlnd",
   "source": "# =============================================================================\n# SECTION 3 — TEMPORAL STRUCTURE PLOT\n# =============================================================================\n\nphase_order  = ['grasping', 'pose', 'stability', 'retract']\nphase_colors = ['#4e9af1', '#f1c84e', '#f17c4e', '#a0d9a0']\nphase_labels = ['Grasp', 'Pose', 'Stability', 'Retract']\n\n# Build (start, end) for each phase from stages\nphase_bounds = {}\nfor i, ph in enumerate(phase_order[:-1]):\n    t_start = stages.get(ph, stages.get('grasping') if ph == 'grasping' else None)\n    t_end   = stages.get(phase_order[i + 1])\n    if t_start is not None and t_end is not None:\n        phase_bounds[ph] = (t_start, t_end)\nlast_ph = phase_order[-1]\nif last_ph in stages:\n    last_end = stages[last_ph] + max(\n        (stages[last_ph] - stages.get(phase_order[-2], stages[last_ph])), 2)\n    phase_bounds[last_ph] = (stages[last_ph], last_end)\n\nt_min = min(v[0] for v in phase_bounds.values())\nt_max = max(v[1] for v in phase_bounds.values())\n\nfig, ax = plt.subplots(figsize=(12, 2.5))\nfor i, (ph, col, lbl) in enumerate(zip(phase_order, phase_colors, phase_labels)):\n    if ph not in phase_bounds:\n        continue\n    t0, t1 = phase_bounds[ph]\n    ax.barh(0, t1 - t0, left=t0, height=0.5, color=col, edgecolor='k', linewidth=0.8)\n    ax.text((t0 + t1) / 2, 0, lbl, ha='center', va='center', fontsize=9, fontweight='bold')\n\n# Mark the training window\nt_win_start = t_grasp\nt_win_end   = (seconds[-1] + 1) if seconds else t_stability\nax.axvline(t_win_start, color='navy', linestyle='--', linewidth=1.5, label='Window start (t_grasp)')\nax.axvline(t_win_end,   color='darkred', linestyle='--', linewidth=1.5, label=f'Window end ({len(seconds)}s clipped)')\n\n# Mark each sampled second as a tick\nfor s in seconds:\n    ax.axvline(s, color='gray', linestyle=':', linewidth=0.6, alpha=0.6)\n\nax.set_xlim(t_min - 1, t_max + 1)\nax.set_yticks([])\nax.set_xlabel(\"Unix timestamp (seconds)\")\nax.set_title(f\"Episode timeline — {sample_dir.name}\")\nax.legend(fontsize=8, loc='upper right')\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nTraining window: {len(seconds)} seconds → T={len(seconds)} LSTM timesteps\")\nprint(f\"  Each timestep contains: F1={F1} image frames + F2={F2} FT readings + F2={F2} gripper readings\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "v7suj9df6v9",
   "source": "# =============================================================================\n# SECTION 4 — GELSIGHT PREPROCESSING (Baseline Subtraction)\n# =============================================================================\n\n# Build second→paths mapping\ngel_by_sec = {}\nfor ts, _, p in gel_triples:\n    gel_by_sec.setdefault(ts, []).append(p)\n\n# --- Baseline: first GelSight frame at t_grasp ---\nbaseline_paths = gel_by_sec.get(t_grasp, [])\nbaseline_pil   = Image.open(baseline_paths[0]).convert('RGB') if baseline_paths else None\nbaseline_t     = _load_image(baseline_paths[0] if baseline_paths else None)\nprint(f\"Baseline frame: {baseline_paths[0].name if baseline_paths else 'NONE (zeros)'}\")\nprint(f\"  Tensor shape:  {baseline_t.shape}  (C, H, W)\")\nprint(f\"  Pixel range:   [{baseline_t.min():.3f}, {baseline_t.max():.3f}]  (after ImageNet normalize)\\n\")\n\n# --- Sample F1=2 frames from seconds[0] ---\nsec0       = seconds[0]\ngel_paths0 = _sample_image_bucket(gel_by_sec.get(sec0, []), f=F1)\nprint(f\"Second {sec0}: {len(gel_by_sec.get(sec0, []))} total frames → \"\n      f\"linspace picks {F1}: {[p.name for p in gel_paths0]}\")\n\n# --- Visualise: raw | baseline | (raw - baseline) for each sampled frame ---\nn_frames = len(gel_paths0) if gel_paths0 else 0\nfig, axes = plt.subplots(n_frames, 3, figsize=(10, 3.5 * n_frames))\nif n_frames == 1:\n    axes = [axes]\n\ndef _to_display(t):\n    \"\"\"Undo ImageNet normalize then clip to [0,1] for imshow.\"\"\"\n    mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n    std  = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n    return (t * std + mean).clamp(0, 1).permute(1,2,0).numpy()\n\nfor i, p in enumerate(gel_paths0):\n    raw_t  = _load_image(p)\n    diff_t = raw_t - baseline_t\n\n    axes[i][0].imshow(_to_display(raw_t));  axes[i][0].set_title(f\"Raw (frame {i})\")\n    axes[i][1].imshow(_to_display(baseline_t)); axes[i][1].set_title(\"Baseline (t_grasp frame 0)\")\n    axes[i][2].imshow(_to_display(diff_t + 0.5)); axes[i][2].set_title(\"Raw − Baseline  (+0.5 offset)\")\n    for ax in axes[i]: ax.axis('off')\n\nplt.suptitle(f\"GelSight — {sample_dir.name}  |  second {sec0}\", fontsize=11)\nplt.tight_layout()\nplt.show()\n\n# Pixel stats\nraw_t = _load_image(gel_paths0[0])\nprint(f\"\\nPixel stats (first sampled frame, after IMG_TRANSFORM normalize):\")\nprint(f\"  raw tensor:       mean={raw_t.mean():.4f}  std={raw_t.std():.4f}\")\ndiff_t = raw_t - baseline_t\nprint(f\"  raw - baseline:   mean={diff_t.mean():.4f}  std={diff_t.std():.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "iwzfqhyduxp",
   "source": "# =============================================================================\n# SECTION 5 — RGB FRAMES\n# =============================================================================\n\nrgb_by_sec = {}\nfor ts, _, p in rgb_triples:\n    rgb_by_sec.setdefault(ts, []).append(p)\n\n# Show F1 sampled RGB frames for the first 4 seconds\nn_show  = min(4, len(seconds))\nfig, axes = plt.subplots(n_show, F1, figsize=(4 * F1, 3.5 * n_show))\nif n_show == 1:\n    axes = [axes]\n\nfor row, sec in enumerate(seconds[:n_show]):\n    available = rgb_by_sec.get(sec, [])\n    sampled   = _sample_image_bucket(available, f=F1)\n    for col in range(F1):\n        ax = axes[row][col] if F1 > 1 else axes[row]\n        if sampled is None or sampled[col] is None:\n            ax.text(0.5, 0.5, 'EMPTY', ha='center', va='center', transform=ax.transAxes)\n        else:\n            ax.imshow(_to_display(_load_image(sampled[col])))\n        ax.set_title(f\"sec={sec}  frame[{col}]  ({len(available)} avail)\", fontsize=8)\n        ax.axis('off')\n\nplt.suptitle(f\"RGB frames — {sample_dir.name}\", fontsize=11)\nplt.tight_layout()\nplt.show()\n\n# Shape trace for one frame\nprint(\"Shape trace: PIL → ToTensor → Normalize → final tensor\")\np = rgb_by_sec.get(seconds[0], [None])[0]\nif p:\n    pil   = Image.open(p).convert('RGB')\n    print(f\"  PIL size:     {pil.size}  (W×H)\")\n    import torchvision.transforms.functional as TF\n    resized = TF.resize(pil, [224, 224])\n    print(f\"  After Resize: {resized.size}\")\n    arr = TF.to_tensor(resized)\n    print(f\"  After ToTensor: {arr.shape}  range=[{arr.min():.3f},{arr.max():.3f}]\")\n    final = IMG_TRANSFORM(pil)\n    print(f\"  After Normalize: {final.shape}  range=[{final.min():.3f},{final.max():.3f}]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4sy7mtshgmc",
   "source": "# =============================================================================\n# SECTION 6 — FORCE-TORQUE DEEP DIVE\n# =============================================================================\n\nft_labels = ['Fx', 'Fy', 'Fz', 'Tx', 'Ty', 'Tz']\n\n# Mask to the training window\nwin_ft_mask = (ft_ts >= t_grasp) & (ft_ts < (seconds[-1] + 1 if seconds else t_stability))\nwin_ft_ts   = ft_ts[win_ft_mask].astype(float)\nwin_ft_val  = ft_val[win_ft_mask]\n\nfig, axes = plt.subplots(2, 3, figsize=(14, 6), sharex=True)\nfor i, (ax, lbl) in enumerate(zip(axes.flat, ft_labels)):\n    ax.plot(win_ft_ts - win_ft_ts[0], win_ft_val[:, i], linewidth=0.8)\n    for s in seconds:\n        ax.axvline(s - t_grasp, color='gray', linestyle=':', linewidth=0.6, alpha=0.7)\n    ax.set_title(lbl)\n    ax.set_xlabel(\"t − t_grasp (s)\")\n    ax.grid(True, alpha=0.3)\nplt.suptitle(f\"Force-Torque timeseries — {sample_dir.name}\", fontsize=11)\nplt.tight_layout()\nplt.show()\n\n# --- Show _sample_bucket logic for one second ---\nprint(\"\\n_sample_bucket demo — F/T for first two seconds:\")\nfor sec in seconds[:2]:\n    mask   = ft_ts == sec\n    bucket = ft_val[mask]            # (k, 6)\n    k      = len(bucket)\n    result = _sample_bucket(bucket, n_cols=6, f=F2)\n    if result is None:\n        status = f\"WARN: k={k} < F2={F2} → sample skipped\"\n    else:\n        idx    = np.round(np.linspace(0, k-1, F2)).astype(int)\n        status = f\"k={k} → linspace picks rows {idx.tolist()} → shape ({F2*6},)\"\n    print(f\"  sec={sec}: {status}\")\n    if result is not None:\n        print(f\"    result[:6] = {result[:6].round(4)}\")\n        print(f\"    result[6:] = {result[6:].round(4)}  (second reading)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "1lsytqkz8mu",
   "source": "# =============================================================================\n# SECTION 7 — GRIPPER STATE\n# =============================================================================\n\nwin_gr_mask = (gr_ts >= t_grasp) & (gr_ts < (seconds[-1] + 1 if seconds else t_stability))\nwin_gr_ts   = gr_ts[win_gr_mask].astype(float)\nwin_gr_val  = gr_val[win_gr_mask]\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 3.5), sharex=True)\nfor i, (ax, lbl) in enumerate(zip(axes, ['Left finger', 'Right finger'])):\n    ax.plot(win_gr_ts - win_gr_ts[0], win_gr_val[:, i], marker='o', markersize=3, linewidth=0.9)\n    for s in seconds:\n        ax.axvline(s - t_grasp, color='gray', linestyle=':', linewidth=0.6, alpha=0.7)\n    ax.set_title(lbl)\n    ax.set_xlabel(\"t − t_grasp (s)\")\n    ax.grid(True, alpha=0.3)\nplt.suptitle(f\"Gripper state — {sample_dir.name}\", fontsize=11)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\ngripper_force (from folder name): {meta['force']} N  →  tensor([{meta['force']:.1f}])\")\n\nprint(\"\\n_sample_bucket demo — gripper for first two seconds:\")\nfor sec in seconds[:2]:\n    mask   = gr_ts == sec\n    bucket = gr_val[mask]\n    k      = len(bucket)\n    result = _sample_bucket(bucket, n_cols=2, f=F2)\n    if result is None:\n        print(f\"  sec={sec}: WARN k={k} < F2={F2}\")\n    else:\n        idx = np.round(np.linspace(0, k-1, F2)).astype(int)\n        print(f\"  sec={sec}: k={k} → picks rows {idx.tolist()} → {result.round(4)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8myogolxqcq",
   "source": "# =============================================================================\n# SECTION 8 — FULL _build_sample TRACE (all shapes)\n# =============================================================================\n\nprint(f\"Building sample from: {sample_dir.name}\")\nprint(\"=\" * 60)\n\ns = _build_sample(sample_dir)\n\nif s is None:\n    print(\"_build_sample returned None — this sample was skipped\")\n    print(\"(likely a bucket with 0 < k < F frames, or missing stages/labels)\")\nelse:\n    tensor_keys = ['tactile', 'rgb', 'ft', 'gripper', 'gripper_force', 'label', 'pose_label']\n    for k in tensor_keys:\n        v = s[k]\n        if hasattr(v, 'shape'):\n            print(f\"  {k:20s}: {str(tuple(v.shape)):30s}  dtype={v.dtype}\")\n        else:\n            print(f\"  {k:20s}: {v}\")\n\n    print(f\"\\n  {'object':20s}: {s['object']}\")\n    print(f\"  {'pose_idx':20s}: {s['pose_idx']}\")\n    print(f\"  {'force':20s}: {s['force']} N\")\n    print(f\"  {'grasp_label':20s}: {s['grasp_label']}  (stored in .samples but NOT in __getitem__)\")\n\n    T  = s['tactile'].shape[0]\n    print(f\"\\nSummary: T={T} seconds\")\n    print(f\"  tactile  : (T={T}, F1={F1}, 3, 224, 224)\")\n    print(f\"  rgb      : (T={T}, F1={F1}, 3, 224, 224)\")\n    print(f\"  ft       : (T={T}, FT_DIM={FT_DIM})\")\n    print(f\"  gripper  : (T={T}, GR_DIM={GR_DIM})\")\n    print(f\"\\n  label (stability/retract) = {s['label'].item()}  {'[PASS]' if s['label']==0 else '[SLIP/DROP]'}\")\n    print(f\"  pose_label               = {s['pose_label'].item()}  {'[PASS]' if s['pose_label']==0 else '[SLIP/DROP]'}\")\n    eq = \"S=\" if s['label'].item() == s['pose_label'].item() else \"S≠\"\n    print(f\"  DRS group                : {eq}  (label {'==' if eq=='S=' else '!='} pose_label)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "z3yf2lulb4o",
   "source": "# =============================================================================\n# SECTION 9 — DATASET LOAD & STATS\n# =============================================================================\n\n# --- Load (subsampled for speed) ---\nds = PoseItDataset(root_dir=ROOT_DIR)\nif SUBSAMPLE < 1.0:\n    k = max(4, int(len(ds.samples) * SUBSAMPLE))\n    ds.samples = random.sample(ds.samples, k)\n    print(f\"Subsampled to {len(ds.samples)} samples\")\n\nprint(f\"\\nTotal samples loaded: {len(ds)}\")\n\n# --- Label distribution ---\nlabels_arr      = np.array([s['label'].item()      for s in ds.samples])\npose_labels_arr = np.array([s['pose_label'].item() for s in ds.samples])\nn_pass  = (labels_arr == 0).sum()\nn_fail  = (labels_arr == 1).sum()\nprint(f\"\\nStability label:  {n_pass} pass ({n_pass/len(ds)*100:.1f}%)  |  {n_fail} slip/drop ({n_fail/len(ds)*100:.1f}%)\")\nn_pp  = (pose_labels_arr == 0).sum()\nn_pf  = (pose_labels_arr == 1).sum()\nprint(f\"Pose label:       {n_pp} pass ({n_pp/len(ds)*100:.1f}%)  |  {n_pf} slip/drop ({n_pf/len(ds)*100:.1f}%)\")\n\nfig, axes = plt.subplots(1, 3, figsize=(14, 4))\n\n# Stability label distribution\naxes[0].bar(['Pass (0)', 'Slip/Drop (1)'], [n_pass, n_fail], color=['#4e9af1', '#f17c4e'])\naxes[0].set_title('Stability label  (training target)')\naxes[0].set_ylabel('Count')\nfor bar, val in zip(axes[0].patches, [n_pass, n_fail]):\n    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3, str(val), ha='center')\n\n# Sequence length distribution\nseq_lens = [s['tactile'].shape[0] for s in ds.samples]\naxes[1].hist(seq_lens, bins=range(1, max(seq_lens)+2), edgecolor='k', color='#a0d9a0')\naxes[1].set_title(f'Sequence length T  (max={max(seq_lens)}, mean={np.mean(seq_lens):.1f})')\naxes[1].set_xlabel('T (seconds)')\naxes[1].set_ylabel('Count')\n\n# Object distribution (top 10)\nfrom collections import Counter\nobj_counts = Counter(s['object'] for s in ds.samples)\ntop_objs   = sorted(obj_counts.items(), key=lambda x: -x[1])[:10]\naxes[2].barh([x[0] for x in top_objs], [x[1] for x in top_objs], color='#c9b0e8')\naxes[2].set_title('Object distribution (top 10)')\naxes[2].set_xlabel('Count')\naxes[2].invert_yaxis()\n\nplt.tight_layout()\nplt.show()\n\n# Force level breakdown\nforce_counts = Counter(s['force'] for s in ds.samples)\nprint(f\"\\nForce levels: { {f'F{int(f)}': c for f, c in sorted(force_counts.items())} }\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "9v0jmfnlz3",
   "source": "# =============================================================================\n# SECTION 10 — SPLITS & DRS SAMPLER\n# =============================================================================\n\n# --- Build split ---\nif SPLIT == 'object':\n    train_set, val_set, test_set = split_by_object(ds, test_objects=TEST_OBJECTS)\nelif SPLIT == 'pose':\n    train_set, val_set, test_set = split_by_pose(ds, test_pose_indices=TEST_POSES)\nelse:\n    train_set, val_set, test_set = uniform_random_split(ds)\n\nprint(f\"Split ({SPLIT}):  train={len(train_set)}  val={len(val_set)}  test={len(test_set)}\")\n\ndef split_labels(dataset, indices):\n    lbl = np.array([dataset.samples[i]['label'].item() for i in indices])\n    n0, n1 = (lbl==0).sum(), (lbl==1).sum()\n    return n0, n1\n\nfor name, subset in [('Train', train_set), ('Val', val_set), ('Test', test_set)]:\n    n0, n1 = split_labels(ds, subset.indices)\n    print(f\"  {name:6s}: {len(subset.indices):4d} samples  —  \"\n          f\"pass={n0} ({n0/max(len(subset.indices),1)*100:.0f}%)  \"\n          f\"slip/drop={n1} ({n1/max(len(subset.indices),1)*100:.0f}%)\")\n\n# --- DRS Sampler ---\nprint()\nsampler = DRSSampler(\n    dataset=ds,\n    sigma=SIGMA,\n    batch_size=BATCH_SIZE,\n    indices=train_set.indices,\n)\nprint(f\"  S=  indices: {len(sampler.s_eq)}\")\nprint(f\"  S≠  indices: {len(sampler.s_neq)}\")\nprint(f\"  r  (natural ratio): {sampler.r:.4f}\")\nprint(f\"  σ  (target ratio):  {SIGMA}\")\nprint(f\"  keep_prob(S=):      {sampler.keep_prob:.4f}\")\n\n# --- Simulate batches before and after DRS activation ---\ndef simulate_batches(sampler, n=20):\n    \"\"\"Return list of (n_eq, n_neq) per batch.\"\"\"\n    stats = []\n    s_neq_set = set(sampler.s_neq.tolist())\n    for batch in list(sampler)[:n]:\n        n_neq = sum(1 for i in batch if i in s_neq_set)\n        n_eq  = len(batch) - n_neq\n        stats.append((n_eq, n_neq))\n    return stats\n\npre_stats = simulate_batches(sampler, n=20)\nsampler.activate()\npost_stats = simulate_batches(sampler, n=20)\n\nfig, axes = plt.subplots(1, 2, figsize=(13, 4))\nfor ax, stats, title in [\n    (axes[0], pre_stats,  \"Before DRS  (uniform batches)\"),\n    (axes[1], post_stats, f\"After DRS  (σ={SIGMA})\"),\n]:\n    n_eq_list  = [s[0] for s in stats]\n    n_neq_list = [s[1] for s in stats]\n    x = range(len(stats))\n    ax.bar(x, n_eq_list,  label='S= (label==pose_label)', color='#4e9af1')\n    ax.bar(x, n_neq_list, bottom=n_eq_list, label='S≠ (label!=pose_label)', color='#f17c4e')\n    ax.set_title(title)\n    ax.set_xlabel(\"Batch index\")\n    ax.set_ylabel(\"# samples\")\n    ax.legend(fontsize=8)\n    avg_neq_frac = np.mean([n1/(n0+n1) if (n0+n1)>0 else 0 for n0,n1 in stats])\n    ax.set_xlabel(f\"Batch index  (avg S≠ fraction: {avg_neq_frac:.2f})\")\n\nplt.suptitle(\"DRS batch composition — S= vs S≠\", fontsize=11)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "djmoe46ysx9",
   "source": "# =============================================================================\n# SECTION 11 — DATALOADER BATCH INSPECTION\n# =============================================================================\n\n# Re-create sampler (deactivated — we want uniform batches here for clarity)\nsampler_inspect = DRSSampler(\n    dataset=ds, sigma=SIGMA, batch_size=BATCH_SIZE, indices=train_set.indices\n)\n\ntrain_loader = DataLoader(\n    ds,\n    batch_sampler=sampler_inspect,\n    num_workers=0,\n    collate_fn=collate_variable_length,\n)\n\nbatch = next(iter(train_loader))\ntac_b, rgb_b, ft_b, grip_b, gf_b, lbl_b, pl_b, lengths_b = batch\n\nprint(\"Collated batch shapes:\")\nprint(f\"  tactile       : {tuple(tac_b.shape)}  (B, max_T, F1, C, H, W)\")\nprint(f\"  rgb           : {tuple(rgb_b.shape)}\")\nprint(f\"  ft            : {tuple(ft_b.shape)}  (B, max_T, FT_DIM={FT_DIM})\")\nprint(f\"  gripper       : {tuple(grip_b.shape)}  (B, max_T, GR_DIM={GR_DIM})\")\nprint(f\"  gripper_force : {tuple(gf_b.shape)}  (B, 1)\")\nprint(f\"  label         : {tuple(lbl_b.shape)}  values={lbl_b.tolist()}\")\nprint(f\"  pose_label    : {tuple(pl_b.shape)}  values={pl_b.tolist()}\")\nprint(f\"  lengths       : {tuple(lengths_b.shape)}  values={lengths_b.tolist()}\")\nprint(f\"\\n  max_T={tac_b.shape[1]}  (padding added to match longest in batch)\")\n\nB = tac_b.shape[0]\nmax_T = tac_b.shape[1]\n\n# --- Visualise padding ---\nfig, ax = plt.subplots(figsize=(9, 3))\ncolors = ['#4e9af1', '#f17c4e', '#a0d9a0', '#f1c84e']\nfor i, (L_i, c) in enumerate(zip(lengths_b.tolist(), colors)):\n    ax.barh(i, L_i,       height=0.5, color=c,       label=f'sample {i}  (T={L_i}, label={lbl_b[i].item()})')\n    ax.barh(i, max_T-L_i, height=0.5, color='#ddd', left=L_i)\nax.set_xlim(0, max_T + 1)\nax.set_yticks(range(B))\nax.set_yticklabels([f\"sample {i}\" for i in range(B)])\nax.set_xlabel(\"Time step\")\nax.set_title(\"Sequence lengths in batch  (grey = zero-padding)\")\nax.legend(fontsize=8, loc='lower right')\nplt.tight_layout()\nplt.show()\n\n# Verify zero-padding: ft at padded positions should be all zeros\nprint(\"\\nZero-padding check (ft values at padded timesteps):\")\nfor i in range(B):\n    L_i = lengths_b[i].item()\n    if L_i < max_T:\n        pad_slice = ft_b[i, L_i:, :]\n        print(f\"  sample {i}: ft[{L_i}:{max_T}] all_zero={pad_slice.abs().max().item() == 0.0}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "5f6hb8hu5wx",
   "source": "# =============================================================================\n# SECTION 12 — MODEL ARCHITECTURE & FORWARD PASS\n# =============================================================================\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nmodel = GraspStabilityLSTM(\n    frames_per_sec=F2,\n    ft_dim=FT_DIM,\n    gripper_dim=GR_DIM,\n    hidden_dim=256,\n    dropout=0.1,\n    modalities=['V', 'T', 'FT', 'G', 'GF'],\n).to(device)\n\n# --- Parameter counts ---\ndef count_params(module):\n    total     = sum(p.numel() for p in module.parameters())\n    trainable = sum(p.numel() for p in module.parameters() if p.requires_grad)\n    return total, trainable\n\ncomponents = [\n    ('rgb_encoder',     model.rgb_encoder),\n    ('tactile_encoder', model.tactile_encoder),\n    ('projection',      model.projection),\n    ('lstm',            model.lstm),\n    ('classifier',      model.classifier),\n]\nprint(f\"{'Component':<22} {'Total':>12} {'Trainable':>12}\")\nprint(\"-\" * 48)\ngrand_total = grand_trainable = 0\nfor name, mod in components:\n    tot, tr = count_params(mod)\n    frozen_tag = '' if tr > 0 else '  [FROZEN]'\n    print(f\"  {name:<20} {tot:>12,} {tr:>12,}{frozen_tag}\")\n    grand_total     += tot\n    grand_trainable += tr\nprint(\"-\" * 48)\nprint(f\"  {'TOTAL':<20} {grand_total:>12,} {grand_trainable:>12,}\")\nprint(f\"\\n  Trainable fraction: {grand_trainable/grand_total*100:.2f}%\")\n\n# --- Forward pass with shape trace ---\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Forward pass shape trace\")\nprint(\"=\" * 60)\n\ntac_b_dev  = tac_b.to(device)\nrgb_b_dev  = rgb_b.to(device)\nft_b_dev   = ft_b.to(device)\ngrip_b_dev = grip_b.to(device)\ngf_b_dev   = gf_b.to(device)\n\nB, T, F1_dim = tac_b_dev.shape[:3]\nprint(f\"\\nInput shapes  (B={B}, T={T}, F1={F1_dim}):\")\nprint(f\"  tactile       : {tuple(tac_b_dev.shape)}\")\nprint(f\"  rgb           : {tuple(rgb_b_dev.shape)}\")\nprint(f\"  ft            : {tuple(ft_b_dev.shape)}\")\nprint(f\"  gripper       : {tuple(grip_b_dev.shape)}\")\nprint(f\"  gripper_force : {tuple(gf_b_dev.shape)}\")\n\nprint(f\"\\nPer-second ResNet encoding:\")\nS = T * F1_dim\nprint(f\"  Reshape tactile: (B*T*F1, 3, 224, 224) = ({B*T*F1_dim}, 3, 224, 224)\")\nprint(f\"  ResNet50 → (B*T*F1, 2048) = ({B*T*F1_dim}, 2048)\")\nprint(f\"  Reshape back: (B, T, F1*2048) = ({B}, {T}, {F1_dim*2048})\")\n\npre_lstm_dim = F1_dim * 2048 * 2 + FT_DIM + GR_DIM + 1\nprint(f\"\\nFusion concat dim: F1*2048*2 + FT_DIM + GR_DIM + 1\")\nprint(f\"  = {F1_dim}*2048*2 + {FT_DIM} + {GR_DIM} + 1 = {pre_lstm_dim}\")\nprint(f\"  After projection: (B, T, hidden_dim=256) = ({B}, {T}, 256)\")\n\nprint(f\"\\nBiLSTM (2-layer, bidirectional):\")\nprint(f\"  Input : (B, T, 256) = ({B}, {T}, 256)\")\nprint(f\"  Output: (B, T, 512) = ({B}, {T}, 512)  [hidden_dim*2 for bidirectional]\")\nprint(f\"  Use last timestep: lstm_out[:, -1, :] → (B, 512)\")\n\nprint(f\"\\nClassifier head:\")\nprint(f\"  Linear(512→64) → ReLU → Dropout → Linear(64→1)\")\nprint(f\"  Output: (B, 1) = ({B}, 1)  [raw logit]\")\n\nmodel.eval()\nwith torch.no_grad():\n    logits = model(tac_b_dev, rgb_b_dev, ft_b_dev, grip_b_dev, gf_b_dev)\nprint(f\"\\nActual output — logits shape: {tuple(logits.shape)}\")\nprint(f\"  logits:          {logits.squeeze(1).cpu().tolist()}\")\nprint(f\"  sigmoid(logits): {logits.squeeze(1).sigmoid().cpu().tolist()}\")\nprint(f\"  predictions:     {(logits.squeeze(1) > 0).cpu().tolist()}  (True=slip/drop)\")\nprint(f\"  ground truth:    {lbl_b.tolist()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "x3djckajpvf",
   "source": "# =============================================================================\n# SECTION 13 — MINI TRAINING LOOP & TRAINING BEHAVIOR DIAGNOSIS\n# =============================================================================\n\nimport torch.nn as nn\n\n# ---- Part A: Overfit sanity check (1 sample, loss must fall to ~0) ----\nprint(\"Part A: Overfit sanity check  (1 sample, 150 iters)\")\nprint(\"=\" * 55)\n\nds_over = PoseItDataset(root_dir=ROOT_DIR)\nds_over.samples = ds_over.samples[:1]\nover_loader = DataLoader(ds_over, batch_size=1, shuffle=False,\n                         num_workers=0, collate_fn=collate_variable_length)\n\nmodel_over = GraspStabilityLSTM(\n    frames_per_sec=F2, ft_dim=FT_DIM, gripper_dim=GR_DIM,\n    hidden_dim=256, dropout=0.0, modalities=['V','T','FT','G','GF'],\n).to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.SGD(\n    filter(lambda p: p.requires_grad, model_over.parameters()),\n    lr=0.001, momentum=0.9, weight_decay=0.0,\n)\n\noverfit_losses = []\noverfit_batch  = next(iter(over_loader))\ntac_o, rgb_o, ft_o, grip_o, gf_o, lbl_o, _, _ = [\n    x.to(device) if hasattr(x,'to') else x for x in overfit_batch\n]\nprint(f\"  Single sample label: {lbl_o.item()}  ({'slip/drop' if lbl_o.item()==1 else 'pass'})\")\n\nmodel_over.train()\nfor it in range(150):\n    optimizer.zero_grad()\n    logit = model_over(tac_o, rgb_o, ft_o, grip_o, gf_o).squeeze(1)\n    loss  = criterion(logit, lbl_o.float())\n    loss.backward()\n    optimizer.step()\n    overfit_losses.append(loss.item())\n    if it % 30 == 0:\n        print(f\"  iter {it:3d}: loss={loss.item():.4f}  \"\n              f\"logit={logit.item():.3f}  pred={'slip' if logit.item()>0 else 'pass'}\")\n\nfig, ax = plt.subplots(figsize=(8, 3))\nax.plot(overfit_losses)\nax.set_xlabel(\"Iteration\")\nax.set_ylabel(\"Loss\")\nax.set_title(\"Overfit check — loss should approach 0\")\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nfinal_loss = overfit_losses[-1]\nstatus = \"PASS\" if final_loss < 0.1 else \"CONCERN (may need more iters or lower LR)\"\nprint(f\"\\n  Final loss: {final_loss:.4f}  [{status}]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "hcy8hljymcl",
   "source": "# ---- Part B: Simulate the stuck-at-50% training behavior & explain it ----\nprint(\"Part B: Training behavior diagnosis\")\nprint(\"=\" * 55)\n\n@torch.no_grad()\ndef quick_eval(model, loader, criterion, device):\n    model.eval()\n    tp, fp, fn, n, total_loss = 0, 0, 0, 0, 0.0\n    for batch in loader:\n        tac, rgb, ft, grip, gf, lbl, _, _ = [\n            x.to(device) if hasattr(x,'to') else x for x in batch\n        ]\n        logits = model(tac, rgb, ft, grip, gf).squeeze(1)\n        total_loss += criterion(logits, lbl.float()).item() * len(lbl)\n        preds  = logits > 0\n        actual = lbl.bool()\n        tp += (preds &  actual).sum().item()\n        fp += (preds & ~actual).sum().item()\n        fn += (~preds & actual).sum().item()\n        n  += len(lbl)\n    if n == 0: return 0,0,0,0,0\n    acc  = (tp + (n - tp - fp - fn)) / n\n    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n    rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n    f1   = 2*prec*rec/(prec+rec) if (prec+rec) > 0 else 0.0\n    return total_loss/n, acc, prec, rec, f1\n\n# Build small multi-sample loaders for the demo\nval_loader_demo = DataLoader(\n    val_set, batch_size=max(1, len(val_set)), shuffle=False,\n    num_workers=0, collate_fn=collate_variable_length,\n)\nsampler_demo = DRSSampler(\n    dataset=ds, sigma=SIGMA, batch_size=BATCH_SIZE, indices=train_set.indices\n)\ntrain_loader_demo = DataLoader(\n    ds, batch_sampler=sampler_demo, num_workers=0, collate_fn=collate_variable_length\n)\n\nmodel_diag = GraspStabilityLSTM(\n    frames_per_sec=F2, ft_dim=FT_DIM, gripper_dim=GR_DIM,\n    hidden_dim=256, dropout=0.1, modalities=['V','T','FT','G','GF'],\n).to(device)\nopt_diag = torch.optim.SGD(\n    filter(lambda p: p.requires_grad, model_diag.parameters()),\n    lr=0.001, momentum=0.9, weight_decay=0.01,\n)\ncrit = nn.BCEWithLogitsLoss()\n\nDEMO_ITERS = 60   # keep short — enough to see the stuck phase\nDRS_AT     = 40   # activate DRS mid-run\n\nhistory = {'iter':[], 'train_loss':[], 'val_loss':[], 'val_acc':[], 'prec':[], 'rec':[], 'f1':[]}\n\nmodel_diag.train()\nit = 0\nfor batch in train_loader_demo:\n    if it >= DEMO_ITERS:\n        break\n    if it == DRS_AT:\n        sampler_demo.activate()\n        print(f\"\\n  [iter {it}] DRS activated  ← batch composition changes here\")\n\n    tac, rgb, ft, grip, gf, lbl, _, _ = [\n        x.to(device) if hasattr(x,'to') else x for x in batch\n    ]\n    opt_diag.zero_grad()\n    logit = model_diag(tac, rgb, ft, grip, gf).squeeze(1)\n    loss  = crit(logit, lbl.float())\n    loss.backward()\n    opt_diag.step()\n\n    if it % 10 == 0:\n        vl, va, vp, vr, vf = quick_eval(model_diag, val_loader_demo, crit, device)\n        history['iter'].append(it);       history['train_loss'].append(loss.item())\n        history['val_loss'].append(vl);   history['val_acc'].append(va)\n        history['prec'].append(vp);       history['rec'].append(vr);  history['f1'].append(vf)\n        print(f\"  iter {it:3d}: train_loss={loss.item():.4f}  val_acc={va*100:.1f}%  \"\n              f\"prec={vp:.3f}  rec={vr:.3f}  f1={vf:.3f}\")\n        model_diag.train()\n    it += 1\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(13, 4))\niters = history['iter']\naxes[0].plot(iters, history['train_loss'], label='train_loss', marker='o')\naxes[0].plot(iters, history['val_loss'],   label='val_loss',   marker='s')\naxes[0].axvline(DRS_AT, color='red', linestyle='--', label=f'DRS on (iter {DRS_AT})')\naxes[0].set_title(\"Loss curves\");  axes[0].legend();  axes[0].grid(True, alpha=0.3)\n\naxes[1].plot(iters, [v*100 for v in history['val_acc']], label='val_acc %', marker='o')\naxes[1].plot(iters, history['prec'], label='precision', marker='s')\naxes[1].plot(iters, history['rec'],  label='recall',    marker='^')\naxes[1].plot(iters, history['f1'],   label='f1',        marker='d')\naxes[1].axvline(DRS_AT, color='red', linestyle='--', label=f'DRS on (iter {DRS_AT})')\naxes[1].set_title(\"Validation metrics\");  axes[1].legend(fontsize=8);  axes[1].grid(True, alpha=0.3)\n\nplt.suptitle(\"Training behavior — stuck phase (DRS off) vs recovery (DRS on)\", fontsize=11)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "2bvfxrqfy7e",
   "source": "# ---- Part C: Training behavior diagnosis printout ----\nprint(\"=\" * 65)\nprint(\"DIAGNOSIS: What you observed in your training run\")\nprint(\"=\" * 65)\n\n# Compute class imbalance from loaded dataset\nn_pass_train = sum(1 for i in train_set.indices if ds.samples[i]['label'].item() == 0)\nn_fail_train = sum(1 for i in train_set.indices if ds.samples[i]['label'].item() == 1)\nn_train = len(train_set.indices)\n\nprint(f\"\"\"\n1. val_acc=50%, prec=0, rec=0, f1=0  (iters 0–299)\n   ─────────────────────────────────────────────────\n   The model collapsed to predicting ALL-NEGATIVE (no slip/drop).\n   \n   Why: Training data is imbalanced.\n     Your run: |train| = {n_train} samples\n               pass (0):      {n_pass_train} ({n_pass_train/n_train*100:.0f}%)\n               slip/drop (1): {n_fail_train} ({n_fail_train/n_train*100:.0f}%)\n   \n   With DRS off, a random batch of 32 likely has ~{int(32*n_fail_train/n_train)} slip/drop\n   samples and ~{int(32*n_pass_train/n_train)} pass. The model finds the trivial local\n   minimum: predict 0 always → achieves ~{n_pass_train/n_train*100:.0f}% train accuracy.\n   \n   Why val_acc=50% (not ~{n_pass_train/n_train*100:.0f}%): The val set (from object split)\n   happens to be ~50% balanced.\n\n2. val_loss monotonically increasing\n   ─────────────────────────────────\n   The model isn't confused; it's growing MORE CONFIDENT in\n   wrong predictions. As logits drift negative, BCE loss on\n   each positive val sample = -log(sigmoid(logit)) keeps rising.\n\n3. train_loss fluctuating\n   ──────────────────────\n   Only the projection+LSTM+classifier are trainable (frozen ResNet).\n   Batch-to-batch variance in the # of slip/drop samples causes\n   noisy gradient estimates → oscillating loss. This is expected\n   with SGD + small batches on imbalanced data.\n\n4. Expected fix at iter 300\n   ────────────────────────\n   DRS activates + LR anneals ×0.1. DRS forces σ={SIGMA} ratio of\n   S≠/S= samples per batch, so the model sees many more slip/drop\n   examples. The lower LR helps escape the trivial minimum.\n   \n   Watch for: recall and f1 becoming non-zero after iter 300.\n   If still stuck at iter 400+, consider:\n     a) Unfreeze last ResNet layer  (freeze_resnet=False or partial)\n     b) Switch to Adam  (more robust to noisy gradients)\n     c) Lower weight_decay  (0.01 may be too aggressive for the small head)\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (temu_conda)",
   "language": "python",
   "name": "temu_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}