{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd8d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"These two below should match\")\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "!which python3\n",
    "print(\"These two above should match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1739030",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL.Image import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30072e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torchsummary torchsummaryX\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde0f52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineTactileEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rgb_freq: int, \n",
    "        tactile_freq: int,\n",
    "        ft_freq: int,\n",
    "        vision_resnet_emb_size: int,\n",
    "        tactile_resnet_emb_size: int,\n",
    "        ft_emb_size: int,\n",
    "        gripper_emb_size: int,\n",
    "        model_emb_size: int,\n",
    "        cell_state_size: int,\n",
    "        hidden_state_size: int):\n",
    "        \n",
    "        super(BaselineTactileEncoder, self).__init__()\n",
    "        self.vision_encoder = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.tactile_encoder = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.vision_resnet_emb_size = vision_resnet_emb_size\n",
    "        self.tactile_resnet_emb_size = tactile_resnet_emb_size\n",
    "        self.model_emb_size = model_emb_size\n",
    "        self.ft_emb_size = ft_emb_size\n",
    "        self.gripper_emb_size = gripper_emb_size\n",
    "        self.hidden_state_size = hidden_state_size\n",
    "        \n",
    "        self.hidden_state = torch.zeros((1, hidden_state_size))\n",
    "        self.cell_state = torch.zeros((1, cell_state_size))\n",
    "        \n",
    "        self._rgb_freq = rgb_freq \n",
    "        self._tactile_freq = tactile_freq\n",
    "        self._ft_freq = ft_freq\n",
    "        \n",
    "        self.resnet_weights = ResNet50_Weights.DEFAULT\n",
    "        self.resnet_transforms = self.resnet_weights.transforms()\n",
    "        # PIL IMAGE (B, C, H, W) -> \n",
    "        # resized to (B, C, 256, 256) -> \n",
    "        # normalized(values[0 to 1], mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "        ## Vision part\n",
    "        self.vision_encoder.fc = nn.Identity() # type: ignore[assignment]\n",
    "\n",
    "\n",
    "        ## Tactile part\n",
    "        self.tactile_encoder.fc = nn.Identity() # type: ignore[assignment]\n",
    "        \n",
    "        self._pre_lstm_dim = (self._rgb_freq    * self.vision_resnet_emb_size + \n",
    "                            self._tactile_freq * self.tactile_resnet_emb_size + \n",
    "                            self._ft_freq      * self.ft_emb_size + \n",
    "                            self._ft_freq * self.gripper_emb_size)\n",
    "\n",
    "        ## pre-lstm projection\n",
    "        self.fc1 = nn.Linear(in_features=self._pre_lstm_dim, \n",
    "                             out_features=self.model_emb_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.model_emb_size, \n",
    "                            hidden_size=self.hidden_state_size, \n",
    "                            num_layers=6,\n",
    "                            bidirectional=False, \n",
    "                            batch_first=True)\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=self.hidden_state_size, \n",
    "                            out_features=self.hidden_state_size//4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn_fc2 = nn.BatchNorm1d(self.hidden_state_size//4)\n",
    "        self.fc3 = nn.Linear(in_features=self.hidden_state_size//4, \n",
    "                            out_features=1)\n",
    "\n",
    "    def forward(self, \n",
    "                vision_img:torch.Tensor,  #---------(f1*T,3,H,W), sampled at f1 fps\n",
    "                tactile_img:torch.Tensor, #---------(f4*T,3,H,W), sampled at f4 fps\n",
    "                ft_data:torch.Tensor, #------(f2*T,6), sampled at f2 fps\n",
    "                gripper_force:torch.Tensor #--(1, 1), sampled at f3 fps\n",
    "                )->torch.Tensor: #---------->> predicted slip probability for time T+1\n",
    "        \n",
    "        # assert vision_img.shape[0] == self._rgb_freq * vision_img.shape[1] // 3, f\"Expected vision_img shape ({self._rgb_freq},3,H,W), got {vision_img.shape}\"\n",
    "        # assert tactile_img.shape[0] == self._tactile_freq * tactile_img.shape[1] // 3, f\"Expected tactile_img shape ({self._tactile_freq},3,H,W), got {tactile_img.shape}\"\n",
    "        # assert ft_data.shape[0] == self._ft_freq * ft_data.shape[1] // 6, f\"Expected ft_data shape ({self._ft_freq},6), got {ft_data.shape}\"\n",
    "        # assert gripper_force.shape[0] == self._ft_freq * gripper_force.shape[1] // 2, f\"Expected gripper_force shape ({self._ft_freq},2), got {gripper_force.shape}\"\n",
    "        \n",
    "        preprocd_vision = self.resnet_transforms(vision_img)\n",
    "        preprocd_tactile = self.resnet_transforms(tactile_img)\n",
    "\n",
    "        vision_emb = self.vision_encoder(preprocd_vision) # (f1*T, vision_resnet_emb_size)\n",
    "        vision_emb = torch.flatten(vision_emb, start_dim=1) # (f1*T*vision_resnet_emb_size)\n",
    "        \n",
    "        tactile_emb = self.tactile_encoder(preprocd_tactile) # (f4*T, tactile_resnet_emb_size)\n",
    "        tactile_emb = torch.flatten(tactile_emb, start_dim=1) # (f4*T*tactile_resnet_emb_size)\n",
    "\n",
    "        ft_emb = torch.flatten(ft_data, start_dim=1) # (f2*T, 6)\n",
    "        gripper_emb = torch.ones((ft_data.shape[0],ft_data.shape[1])) * gripper_force.squeeze(dim=-1)\n",
    "\n",
    "        ## concatenate all the embeddings\n",
    "        combined_emb = torch.cat((vision_emb, tactile_emb, ft_emb, gripper_emb), dim=-1) # (f1*T*vision_resnet_emb_size + f4*T*tactile_resnet_emb_size + f2*T*6 + f3*T*2)\n",
    "\n",
    "        assert combined_emb.shape[1] == self._pre_lstm_dim, f\"Expected combined_emb shape ({self._pre_lstm_dim}), got {combined_emb.shape[1]}\"\n",
    "        \n",
    "        fc1_out = self.fc1(combined_emb) # (model_emb_size)\n",
    "        if(self.hidden_state.shape[0] != fc1_out.shape[0]):\n",
    "            self.hidden_state = self.hidden_state.repeat((fc1_out.shape[0], 1))\n",
    "        \n",
    "        #TODO @ranais: Remove this manual expansion to match L = 1\n",
    "        self.hidden_state = self.hidden_state.unsqueeze(dim=1) # (batch_size, 1, hidden_state_size)\n",
    "        fc1_out = fc1_out.unsqueeze(dim=1) # (batch_size, 1, model_emb_size)\n",
    "        \n",
    "        _ , (lstm_h, lstm_c) = self.lstm(self.hidden_state, fc1_out)\n",
    "        # lstm_h shape: (batch_size, NUM_LSTM_LAYERS, hidden_state_size)\n",
    "        # lstm_c shape: (batch_size, NUM_LSTM_LAYERS, hidden_state_size)\n",
    "        lstm_out = torch.concat(lstm_h[:, -1, :], lstm_c[:, -1, :]) # (batch_size, hidden_state_size)\n",
    "        fc2_out = self.fc2(lstm_out) # (hidden_state_size//4)\n",
    "        fc2_out = self.relu(fc2_out)\n",
    "        fc2_out = self.bn_fc2(fc2_out)\n",
    "        \n",
    "        fc3_out = self.fc3(fc2_out) # (1)\n",
    "        slip_prob = torch.sigmoid(fc3_out) # (1)\n",
    "        return slip_prob\n",
    "        \n",
    "        \n",
    "        \n",
    "        # TODO ranais, For Feb 14th\n",
    "        # 1. Need to add relu, dropout, bn, and complete forward\n",
    "        # 2. Need to figure out what all the shapes will be and try to overfit with a limited set of images\n",
    "        # 3. Does the current architecture make sense? is the flattening correct? Do we need to do some sort of pooling instead of flattening? Do we need SOO many fc layers?\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fd1d399",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineTactileEncoder(\n",
    "    rgb_freq=1,\n",
    "    tactile_freq=1,\n",
    "    ft_freq=10,\n",
    "    vision_resnet_emb_size=2048,\n",
    "    tactile_resnet_emb_size=2048,\n",
    "    ft_emb_size=6,\n",
    "    gripper_emb_size=1,\n",
    "    model_emb_size=4096,\n",
    "    cell_state_size=1024,\n",
    "    hidden_state_size=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92300fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cpu')\n",
    "summary(model, input_size=[(3, 244, 244), (3, 244, 244), (10, 6), (1, 1)] , device='cpu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (temu_conda)",
   "language": "python",
   "name": "temu_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
